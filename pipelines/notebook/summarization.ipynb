{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bba5d53",
   "metadata": {},
   "source": [
    "## Summarization Pipeline\n",
    "> In this pipeline we are using BART for summarizing transcript chunks over 1000 or 800 and refining it using LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af73a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "from langchain_core.documents import Document\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import List\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama-3.3-70b-versatile\", # llama-3.3-70b-versatile good for summarizing texts and providing good context\n",
    "    temperature=0.0,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "hf_client = InferenceClient( # Using facebook/bart-large-cnn\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=HF_TOKEN\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "refine_template = PromptTemplate(template=\"\"\"You are an expert meeting analyst. Your task is to create a comprehensive meeting summary.\n",
    "\n",
    "Existing summary (if any):\n",
    "{existing_answer}\n",
    "\n",
    "New information from the meeting:\n",
    "{text}\n",
    "\n",
    "Instructions:\n",
    "- Combine the existing summary with the new information\n",
    "- Create a concise yet comprehensive summary\n",
    "- Focus on key decisions, action items, and important discussion points\n",
    "- Maintain a professional, clear tone\n",
    "- Organize information logically\n",
    "\n",
    "Refined Summary\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc442a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: BART Summarization Function\n",
    "def summarize_chunk_with_bart(chunk_text: str, max_length: int = 150, min_length: int = 50) -> str:\n",
    "    \"\"\"Summarize a single chunk using BART model via HuggingFace Inference.\"\"\"\n",
    "    try:\n",
    "        summary = hf_client.summarization(\n",
    "            chunk_text,\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "        )\n",
    "        return summary.summary_text if hasattr(summary, 'summary_text') else summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing chunk: {e}\")\n",
    "        return chunk_text[:200]  # Fallback to truncated text\n",
    "\n",
    "# Step 3: Process all chunks with BART\n",
    "def summarize_chunks_with_bart(chunks: List[str]) -> List[str]:\n",
    "    \"\"\"Summarize all chunks using BART.\"\"\"\n",
    "    bart_summaries = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunks)} with BART...\")\n",
    "        summary = summarize_chunk_with_bart(chunk)\n",
    "        bart_summaries.append(summary)\n",
    "        time.sleep(0.6)  # Rate limiting\n",
    "    \n",
    "    return bart_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa43084",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_template = \"\"\"You are an expert meeting analyst. Summarize the following meeting content:\n",
    "\n",
    "{text}\n",
    "\n",
    "Create a concise, detailed summary focusing on:\n",
    "- Key discussion points\n",
    "- Decisions made\n",
    "- Action items\n",
    "- Important insights\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "initial_prompt = PromptTemplate(\n",
    "    template=initial_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Step 5: Main Pipeline Function\n",
    "def summarize_meeting_transcript(transcript: str) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline to summarize meeting transcript.\n",
    "    \n",
    "    Args:\n",
    "        transcript: The full meeting transcript text\n",
    "        \n",
    "    Returns:\n",
    "        dict containing:\n",
    "            - final_summary: The comprehensive final summary\n",
    "            - bart_summaries: Individual BART summaries of chunks\n",
    "            - chunk_count: Number of chunks processed\n",
    "    \"\"\"\n",
    "    print(\"Starting meeting transcript summarization pipeline...\")\n",
    "    \n",
    "    # Split transcript into chunks\n",
    "    print(\"\\n[1/3] Splitting transcript into chunks...\")\n",
    "    chunks = text_splitter.split_text(transcript)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Summarize each chunk with BART\n",
    "    print(\"\\n[2/3] Summarizing chunks with BART...\")\n",
    "    bart_summaries = summarize_chunks_with_bart(chunks)\n",
    "    \n",
    "    # Combine BART summaries into Documents for LangChain\n",
    "    print(\"\\n[3/3] Combining summaries with LLM using refine chain...\")\n",
    "    bart_summary_docs = [Document(page_content=summary) for summary in bart_summaries]\n",
    "    \n",
    "    # Create refine chain\n",
    "    refine_chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"refine\",\n",
    "        question_prompt=initial_prompt,\n",
    "        refine_prompt=refine_template,\n",
    "        return_intermediate_steps=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Generate final summary\n",
    "    result = refine_chain.invoke({\"input_documents\": bart_summary_docs})\n",
    "    final_summary = result[\"output_text\"]\n",
    "    \n",
    "    print(\"\\nâœ“ Pipeline complete!\")\n",
    "    \n",
    "    return {\n",
    "        \"final_summary\": final_summary,\n",
    "        \"bart_summaries\": bart_summaries,\n",
    "        \"chunk_count\": len(chunks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2550d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"Mock Meeting Transcript: Q3 Feature Alignment\n",
    "\n",
    "Meeting Title: Feature Phoenix - Rollout Strategy and Dependencies Date: Tuesday, November 25, 2025 Attendees: Alhaan (Tech Lead), Sofia (Product Manager), David (Marketing Lead), Ms. Mern (Developer) Duration: 30 Minutes\n",
    "\n",
    "00:00 Alhaan: Good afternoon, everyone. This meeting is to finalize the launch timeline for Feature Phoenix and confirm all marketing, tech, and documentation dependencies are aligned. Sofia, can you start with a quick overview of the final user story?\n",
    "\n",
    "01:30 Sofia: Absolutely. Feature Phoenix is a new analytics dashboard allowing our premium users to track their usage efficiency in real-time. The core user story is complete; the Dev team is now integrating the final data pipeline endpoints. The success metric is 20% adoption in the first month.\n",
    "\n",
    "03:00 Alhaan: Thanks. Ms. Mern, how is the pipeline integration looking? Any roadblocks now that the FastAPI auth is stabilized?\n",
    "\n",
    "04:30 Ms. Mern: The FastAPI portion is solid. We've defined the three necessary endpoints: /api/efficiency/daily, /api/efficiency/weekly, and /api/efficiency/summary. I estimate we need three more full development days to complete robust unit testing and deploy to the Staging environment.\n",
    "\n",
    "06:30 Alhaan: Okay, so we're looking at Staging deployment by the end of Friday, November 28th. David, does that give Marketing enough time to prepare for the launch on Tuesday, December 3rd?\n",
    "\n",
    "08:00 David: Launching on December 3rd requires the final UI/UX screenshots and a confirmed feature name by end of day, November 29th, so we can schedule the email campaign and update the landing page. Our dependency is the final, stable Staging link for screenshot capture.\n",
    "\n",
    "10:00 Alhaan: Understood. Ms. Mern, can we promise a stable, QA-approved Staging link by Saturday morning? We need to lock down those front-end assets.\n",
    "\n",
    "11:00 Ms. Mern: Yes, I can commit to having the Staging environment stable and QA-ready by Friday evening, November 28th. I will notify David personally when it's live.\n",
    "\n",
    "12:00 Sofia: Excellent. My team will start drafting the internal documentation and training material based on the current feature set. Alhaan, do you foresee any major risk in the data model scaling beyond initial rollout?\n",
    "\n",
    "14:00 Alhaan: Low risk for V1. The complexity of the current algorithm is manageable for our projected load. We'll start architectural planning for V2 scaling next quarter.\n",
    "\n",
    "15:00 Alhaan: Summary of Action Items: 1. Ms. Mern to provide stable Staging link to David by Nov 28th, 7 PM. 2. David to finalize all launch creative (screenshots, name) by Nov 29th. 3. Sofia to complete internal training docs by Dec 2nd. Launch date remains Dec 3rd.\n",
    "\n",
    "16:00 Alhaan: David, please send out the official launch countdown. Sofia, let's sync on documentation access tomorrow. Thank you all.\n",
    "\n",
    "16:15 Alhaan: Thanks, team. Let's make this a great day.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c3070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting meeting transcript summarization pipeline...\n",
      "\n",
      "[1/3] Splitting transcript into chunks...\n",
      "Created 4 chunks\n",
      "\n",
      "[2/3] Summarizing chunks with BART...\n",
      "Summarizing chunk 1/4 with BART...\n"
     ]
    }
   ],
   "source": [
    "result = summarize_meeting_transcript(transcript)\n",
    "print(result[\"final_summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
