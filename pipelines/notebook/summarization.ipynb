{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bba5d53",
   "metadata": {},
   "source": [
    "## Summarization Pipeline\n",
    "> In this pipeline we are using BART for summarizing transcript chunks over 1000 or 800 and refining it using LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "from langchain_core.documents import Document\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import List\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama-3.3-70b-versatile\", # llama-3.3-70b-versatile good for summarizing texts and providing good context\n",
    "    temperature=0.0,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "hf_client = InferenceClient( # Using facebook/bart-large-cnn\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=HF_TOKEN\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "refine_template = PromptTemplate(template=\"\"\"You are an expert meeting analyst. Your task is to create a comprehensive meeting summary.\n",
    "\n",
    "Existing summary (if any):\n",
    "{existing_answer}\n",
    "\n",
    "New information from the meeting:\n",
    "{text}\n",
    "\n",
    "Instructions:\n",
    "- Combine the existing summary with the new information\n",
    "- Create a concise yet comprehensive summary\n",
    "- Focus on key decisions, action items, and important discussion points\n",
    "- Maintain a professional, clear tone\n",
    "- Organize information logically\n",
    "\n",
    "Refined Summary\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc442a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: BART Summarization Function\n",
    "def summarize_chunk_with_bart(chunk_text: str, max_length: int = 150, min_length: int = 50) -> str:\n",
    "    \"\"\"Summarize a single chunk using BART model via HuggingFace Inference.\"\"\"\n",
    "    try:\n",
    "        summary = hf_client.summarization(\n",
    "            chunk_text,\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            max_length=max_length,\n",
    "            min_length=min_length\n",
    "        )\n",
    "        return summary.summary_text if hasattr(summary, 'summary_text') else summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing chunk: {e}\")\n",
    "        return chunk_text[:200]  # Fallback to truncated text\n",
    "\n",
    "# Step 3: Process all chunks with BART\n",
    "def summarize_chunks_with_bart(chunks: List[str]) -> List[str]:\n",
    "    \"\"\"Summarize all chunks using BART.\"\"\"\n",
    "    bart_summaries = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunks)} with BART...\")\n",
    "        summary = summarize_chunk_with_bart(chunk)\n",
    "        bart_summaries.append(summary)\n",
    "        time.sleep(0.6)  # Rate limiting\n",
    "    \n",
    "    return bart_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa43084",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_template = \"\"\"You are an expert meeting analyst. Summarize the following meeting content:\n",
    "\n",
    "{text}\n",
    "\n",
    "Create a concise, detailed summary focusing on:\n",
    "- Key discussion points\n",
    "- Decisions made\n",
    "- Action items\n",
    "- Important insights\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "initial_prompt = PromptTemplate(\n",
    "    template=initial_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Step 5: Main Pipeline Function\n",
    "def summarize_meeting_transcript(transcript: str) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline to summarize meeting transcript.\n",
    "    \n",
    "    Args:\n",
    "        transcript: The full meeting transcript text\n",
    "        \n",
    "    Returns:\n",
    "        dict containing:\n",
    "            - final_summary: The comprehensive final summary\n",
    "            - bart_summaries: Individual BART summaries of chunks\n",
    "            - chunk_count: Number of chunks processed\n",
    "    \"\"\"\n",
    "    print(\"Starting meeting transcript summarization pipeline...\")\n",
    "    \n",
    "    # Split transcript into chunks\n",
    "    print(\"\\n[1/3] Splitting transcript into chunks...\")\n",
    "    chunks = text_splitter.split_text(transcript)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Summarize each chunk with BART\n",
    "    print(\"\\n[2/3] Summarizing chunks with BART...\")\n",
    "    bart_summaries = summarize_chunks_with_bart(chunks)\n",
    "    \n",
    "    # Combine BART summaries into Documents for LangChain\n",
    "    print(\"\\n[3/3] Combining summaries with LLM using refine chain...\")\n",
    "    bart_summary_docs = [Document(page_content=summary) for summary in bart_summaries]\n",
    "    \n",
    "    # Create refine chain\n",
    "    refine_chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"refine\",\n",
    "        question_prompt=initial_prompt,\n",
    "        refine_prompt=refine_template,\n",
    "        return_intermediate_steps=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Generate final summary\n",
    "    result = refine_chain.invoke({\"input_documents\": bart_summary_docs})\n",
    "    final_summary = result[\"output_text\"]\n",
    "    \n",
    "    print(\"\\nâœ“ Pipeline complete!\")\n",
    "    \n",
    "    return {\n",
    "        \"final_summary\": final_summary,\n",
    "        \"bart_summaries\": bart_summaries,\n",
    "        \"chunk_count\": len(chunks)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
